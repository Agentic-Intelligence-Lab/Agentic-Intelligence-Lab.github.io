<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://agentic-intelligence-lab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://agentic-intelligence-lab.org/" rel="alternate" type="text/html" /><updated>2025-08-15T02:46:53+00:00</updated><id>http://agentic-intelligence-lab.org/feed.xml</id><title type="html">Agentic Intelligence Lab</title><subtitle>Our team at the Department of Data and Systems Engineering, The University of Hong Kong, is dedicated to advancing research in Machine Learning, Sequential Decision Making, and Embodied AI.</subtitle><entry><title type="html">Verlog - A Multi-turn RL framework for LLM agents</title><link href="http://agentic-intelligence-lab.org/2025/08/01/technical-post.html" rel="alternate" type="text/html" title="Verlog - A Multi-turn RL framework for LLM agents" /><published>2025-08-01T00:00:00+00:00</published><updated>2025-08-15T02:45:07+00:00</updated><id>http://agentic-intelligence-lab.org/2025/08/01/technical-post</id><content type="html" xml:base="http://agentic-intelligence-lab.org/2025/08/01/technical-post.html"><![CDATA[<div style="display: flex; justify-content: center; align-items: center; flex-wrap: wrap; gap: 24px; width: 100%;">
  <a href="https://github.com/WentseChen/Verlog" target="_blank" style="text-decoration: none; display: flex; align-items: center;">
    <img src="https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/github.svg" alt="GitHub" width="30" height="30" style="vertical-align: middle;" />
    <span style="vertical-align: middle; font-size: 16px; margin-left: 6px;">Source Code</span>
  </a>

  <a href="https://wandb.ai/cwz19/verlog?nw=nwusercwz19" target="_blank" style="text-decoration: none; display: flex; align-items: center;">
    <img src="https://raw.githubusercontent.com/wandb/assets/main/wandb-dots-logo.svg" alt="W&amp;B" width="30" height="30" style="vertical-align: middle;" />
    <span style="vertical-align: middle; font-size: 16px; margin-left: 6px;">Experiment Logs</span>
  </a>
</div>

<p>Verlog is a multi-turn reinforcement learning framework built for <strong>long-horizon LLM-agentic tasks with highly variable episode lengths</strong>. Extending <a href="https://github.com/volcengine/verl">VeRL</a> and <a href="https://github.com/balrog-ai/BALROG">BALROG</a> while following the proven design principles of <a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail">pytorch-a2c-ppo-acktr-gail</a>, it introduces specialized optimizations for stable and efficient training when episodes span from short interactions to hundreds of turns. Whereas prior frameworks like <a href="https://github.com/volcengine/verl">VeRL</a> and <a href="https://ragen-ai.github.io">RAGEN</a> effectively handle tasks with ~10 turns, and <a href="https://github.com/langfengQ/verl-agent">verl-agent</a> scales up to 50 turns, Verlog is designed to operate in environments with <strong>over 400 turns</strong>, making it uniquely suited for complex, long-term decision-making. This capability has been validated across challenging domains such as BabyAI, BabaIsAI, and Crafter, where it consistently achieves strong performance out of the box. In Crafter, for instance, episode lengths range from 70 to 400 steps with an average of about 190.</p>

<h2 id="key-features">Key features:</h2>

<p>🧠 Turn-Level Abstraction: To handle extremely long episodes, we treat each turn as an independent training sample. This eliminates the need to encode the entire trajectory into a single context window and allows for modular, customizable <a href="#memory-mechanism">memory architectures</a>.</p>

<p>🎯 Fixed-Turn Batching: To address the high variance in episode lengths across environments, we use <a href="#batch-environment-fixed-turn-batching">fixed-turn batching</a>. Each training batch contains a fixed number of turns. For incomplete episodes, we replace final rewards with value function estimates as the supervision signal.</p>

<p>🛠️ Tailored for Multi-Turn RL: To address the unique challenges of multi-turn RL, we introduce a set of targeted techniques such as <a href="#dual-discounting-gae">Dual Discounting GAE</a> and <a href="#critic-warmup">Critic Pre-training</a>, combined with carefully tuned hyperparameters to ensure efficient and stable learning.</p>

<h2 id="main-results">Main Results</h2>

<h3 id="crafter-results">Crafter Results:</h3>

<div style="display: flex; justify-content: space-between; align-items: flex-start; width: 100%; flex-wrap: nowrap;">
  <figure style="flex: 1; text-align: center; margin: 0 10px 20px 10px;">
    <img src="images/crafter_zeroshot.gif" style="width: 100%; height: auto; max-width: none; display: block; margin: 0;" />
    <figcaption style="margin: 0; padding: 0; font-size: 14px; line-height: 1;">Zero-shot policy</figcaption>
  </figure>
  <figure style="flex: 1; text-align: center; margin: 0 10px 20px 10px;">
    <img src="images/crafter_finetuned.gif" style="width: 100%; height: auto; max-width: none; display: block; margin: 0;" />
    <figcaption style="margin: 0; padding: 0; font-size: 14px; line-height: 1;">Fine-tuned policy</figcaption>
  </figure>
</div>

<div style="overflow-x: auto;">
  <table style="width: 100% !important; min-width: 300px !important; border-collapse: collapse !important; font-family: sans-serif !important;">
      <thead>
          <tr>
              <th style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 2px solid #cccccc !important; background-color: #cccccc !important; color: #000 !important;">Metric</th>
              <th style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 2px solid #cccccc !important; background-color: #cccccc !important; color: #000 !important;">Instruct-model</th>
              <th style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 2px solid #cccccc !important; background-color: #cccccc !important; color: #000 !important;">Verlog (Ours)</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 1px solid #cccccc !important;">Rewards</td>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 1px solid #cccccc !important;">5.80</td>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; border-bottom: 1px solid #cccccc !important; font-weight: bold !important;">10.44</td>
          </tr>
          <tr>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important;">Trajectory Length</td>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important;">172.23</td>
              <td style="padding: 8px !important; text-align: center !important; white-space: nowrap !important; font-weight: bold !important;">196.42</td>
          </tr>
      </tbody>
  </table>
</div>

<blockquote>
  <p>Crafter’s experiments are done with Qwen2.5-7B-Instruct model, using PPO algorithm, trained on 8xH100 GPUs with 82Gb memory for ~36 hours, corresponding to 170 PPO updates.</p>
</blockquote>

<div style="display: flex; justify-content: space-between; align-items: flex-start; width: 100%; gap: 20px;">
  <figure style="flex: 1; text-align: center; margin: 0;">
    <img src="images/babyai.gif" style="width: 100%; height: 300px; object-fit: contain; display: block;" />
    <figcaption style="font-size: 14px; line-height: 1;">BabyAI open</figcaption>
  </figure>
  <figure style="flex: 1; text-align: center; margin: 0;">
    <img src="images/babaisai.gif" style="width: 100%; height: 300px; object-fit: contain; display: block;" />
    <figcaption style="font-size: 14px; line-height: 1;">BabaIsAI two_room-maybe_break_stop-goto_win</figcaption>
  </figure>
</div>

<h3 id="babaisai-results-win-rate">BabaIsAI Results (win rate)</h3>

<p>goto_win → 🏁; 
distr_obj → 🎁; 
two_room → 🚪; 
distr_obj_rule → 📏;<br />
maybe_break_stop → ⚠️;</p>

<div style="overflow-x: auto;">
    <table style="width: 100%; border-collapse: collapse; text-align: center; font-family: sans-serif;">
        <thead>
            <tr>
                <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">Model</th>
                <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">🏁+🎁</th>
                <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">🚪+🏁</th>
                <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">🚪+🏁+📏</th>
                <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">🚪+⚠️+🏁</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">Instruct-model</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.66 &plusmn; 0.08</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.03 &plusmn; 0.03</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.22 &plusmn; 0.07</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.19 &plusmn; 0.07</td>
            </tr>
            <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">Verlog (Ours)</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">1.00 &plusmn; 0.00</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">0.89 &plusmn; 0.17</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">0.89 &plusmn; 0.11</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">0.36 &plusmn; 0.07</td>
            </tr>
        </tbody>
    </table>
</div>

<blockquote>
  <p>BabaIsAI’s experiments are done with Qwen2.5-3B-Instruct model, using PPO algorithm, trained on 4xA40 GPUs with 48Gb memory for ~24 hours, corresponding to 300 PPO updates. The maximum episode length was set to 100.</p>
</blockquote>

<h3 id="babyai-results-win-rate">BabyAI Results (win rate)</h3>

<div style="overflow-x: auto;">
  <table style="width: 100%; border-collapse: collapse; text-align: center; font-family: sans-serif;">
      <thead>
          <tr>
              <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">Model</th>
              <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">goto</th>
              <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">pickup</th>
              <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">pick_up_seq_go_to</th>
              <th style="padding: 10px; border-bottom: 2px solid #ddd; background-color: #cccccc !important; color: #000;">open</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td style="padding: 10px; border-bottom: 1px solid #ddd;">Instruct-model</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.88 &plusmn; 0.06</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.41 &plusmn; 0.09</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.22 &plusmn; 0.07</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd;">0.09 &plusmn; 0.05</td>
          </tr>
          <tr>
              <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">Verlog (Ours)</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">1.00 &plusmn; 0.00</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">1.00 &plusmn; 0.00</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">0.65 &plusmn; 0.16</td>
              <td style="padding: 10px; border-bottom: 1px solid #ddd; font-weight: bold;">0.94 &plusmn; 0.07</td>
          </tr>
      </tbody>
  </table>
</div>

<blockquote>
  <p>BabyAI’s experiments are done with Qwen2.5-3B-Instruct model, using PPO algorithm, trained on 4xA40 GPUs with 48Gb memory for ~24 hours, corresponding to 300 PPO updates. The maximum episode length was set to 128.</p>
</blockquote>

<h2 id="technical-report">Technical Report</h2>

<p>In the following sections, we outline our design choices, implementation details, and explore potential research questions that our framework may help address.</p>

<h3 id="model--prompt">Model &amp; Prompt</h3>

<h4 id="instruct-model">Instruct Model</h4>

<p>We begin with the Instruct variant of Qwen-2.5 (Qwen-2.5-3B/7B-Instruct), rather than the base model, for two key reasons. First, it enables seamless integration with <a href="https://github.com/balrog-ai/BALROG">BALROG</a>, a framework designed to evaluate the zero-shot performance of instruct models across a range of benchmarks. Second, it allows us to use the benchmark’s prompts with minimal modifications</p>

<h4 id="memory-mechanism">Memory Mechanism</h4>

<p>Rather than placing the entire trajectory into the context window, we include only the latest $$n+1$$ turns. Each turn, i.e., data = $$(\text{history}<em>t, s_t,$$ $$\text{think}_t, a_t)$$ , with $$\text{history}_t = {s</em>{t-n},$$ $$\text{think}<em>{t-n}, a</em>{t-n},$$ $$…, s_{t-1},$$ $$\text{think}<em>{t-1}, a</em>{t-1}}$$, is treated as an individual training data point. As a result, each training batch consists of <code class="language-plaintext highlighter-rouge">batch_size</code> individual turns, not <code class="language-plaintext highlighter-rouge">batch_size</code> full trajectories.</p>

<p>The results show that for the 3B Qwen model, performance peaks at $$n = 1$$ or $$2$$ and degrades as $$n$$ increases to $$4$$ or $$8$$. We hypothesize that this decline is due to the 3B model’s limited capacity to handle long contexts—for example, $$n = 8$$ yields a prompt of approximately 4.6k tokens. Whether this trend holds for larger models is an open question. Notably, the tasks we evaluate can be framed as Markov Decision Processes (MDPs). In more complex or partially observable tasks, a larger $$n$$ may help.</p>

<p>We observed two notable issues related to the multi-turn memory mechanism:</p>

<ul>
  <li>
    <p><strong>Mimicking prior reasoning patterns:</strong> The model tends to replicate reasoning styles from earlier turns, reducing the diversity of its thought processes.</p>
  </li>
  <li>
    <p><strong>Multi-turn hallucinations:</strong> The model struggles to distinguish between actions imagined during reasoning and actual events in the environment. For example, it may plan to “chop a tree then craft a pickaxe” but fail to find a tree in reality—yet still act as if the plan succeeded. This is a unique challenge for agentic tasks.</p>
  </li>
</ul>

<p>We conducted preliminary experiments to address these issues: (1) We tested a variant that includes only the final action in history: data = $$(\text{history}<em>t, s_t, \text{think}_t, a_t)$$, with $$\text{history}_t = {s</em>{t-n}, a_{t-n}, …, s_{t-1}, a_{t-1}}$$. (2) We tested a variant that periodically clears the history buffer (every 5 steps). Both approaches led to worse performance.</p>

<h4 id="prompt-template">Prompt Template</h4>

<p>Belows is the prompt template used for BabyAI. The prompts are adapted from <a href="https://github.com/balrog-ai/BALROG">BALROG</a>.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[SYSTEM] You are an agent playing a simple navigation game. Your goal is to {MISSION}. The following are the possible actions you can take in the game, followed by a short description of each action: {AVAILABLE ACTIONS}. In a moment I will present you an observation. Tips: {TIPS}. PLAY!
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[USER] {OBSERVATION}
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ASSISTANT] THINK: {THINK} ACTION: {ACTION}
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[USER] {OBSERVATION}. What will you do next? Please respond in the following format: THINK: step-by-step reasoning. ACTION: One valid action from the allowed set.
</code></pre></div></div>
<p>We recommend always examining the model’s zero-shot outputs before training. Specifically, evaluate: (1) Whether reasoning paths are diverse, (2) whether the model reasons sufficiently before selecting an action, (3) the ratio of valid actions, and (4) the types of failure cases. These checks ensure the model understands the environment from the prompt. If not, revise the prompt before fine-tuning.</p>

<h3 id="environment">Environment</h3>

<p>Verlog uses a highly abstract game as its testbed, reducing the need for prompt engineering and allowing researchers to focus on algorithmic design. We detail all engineering aspects below:</p>

<h4 id="valid-action">Valid Action</h4>

<p>Improving the valid action ratio through prompt engineering is the simplest and most effective way to boost performance. In our setup, we ensure the model produces valid actions over 95% of the time using the following strategies:</p>

<ul>
  <li>
    <p>Hardcoded action translation: Certain invalid actions are frequently produced by zero-shot LLMs (e.g., “Move forward” and “Go forward”). We implement a hand-crafted translation function to map these to valid actions, preventing them from lowering the valid action ratio.</p>
  </li>
  <li>
    <p>Replace invalid actions with a default action: When the LLM outputs an invalid action, the environment rejects it and executes a predefined default action instead. Simultaneously, we replace the invalid action with the default one before appending it to the history buffer. This prevents the agent from mimicking the invalid action in subsequent steps.</p>
  </li>
</ul>

<p>We observe that truncating the trajectory upon encountering an invalid action leads to worse performance. Replacing invalid actions with a default action yields better results. In this work, we apply a 0.1 penalty to invalid actions. However, with a high valid action ratio, the format penalty has minimal impact on overall performance.</p>

<h4 id="reward">Reward</h4>

<p>Rewards are rule-based and provided by the environment. In BabyAI and BabaIsAI, we adopt a binary trajectory-level reward scheme: 1 for success trajectory, 0 for failure trajectory. Combined with dual-discount GAE, this setup ensures that earlier steps in suboptimal trajectories receive lower credit compared to those in optimal ones. For Crafter, we use the native environment rewards directly.</p>

<p>We observed a frustrating issue when training on Crafter:</p>

<ul>
  <li>the score improvement mainly comes from reinforcing skills that the base model already possessed, rather than learning new skills. For example, before fine-tuning, agents rarely placed furnace, but after fine-tuning, they successfully place furnace almost every episode. However, skills that were previously unknown, such as crafting an iron sword, remain unlearned even after fine-tuning. This suggests that the current version of RL fails to teach agents new skills on these tasks, instead primarily sharpening the action distribution to favor behaviors with higher immediate rewards.</li>
</ul>

<h4 id="batch-environment-fixed-turn-batching">Batch Environment (Fixed-Turn Batching)</h4>

<p><img src="images/system.jpg" alt="Description" style="max-width:100%; height:auto;" /></p>

<p>Our framework supports asynchronous rollouts and works with any environment using the OpenAI Gym interface. Each training batch size is: <code class="language-plaintext highlighter-rouge">n_env</code> × <code class="language-plaintext highlighter-rouge">e_len</code>, where:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">n_env</code> = number of parallel environments</li>
  <li><code class="language-plaintext highlighter-rouge">e_len</code> = episode length per rollout</li>
</ul>

<p>Note: <code class="language-plaintext highlighter-rouge">e_len</code> can be smaller than the environment’s trajectory length. For example, we set <code class="language-plaintext highlighter-rouge">e_len = 8</code> and max trajectory length = 128 in BabyAI. For early truncated trajectories, we leverage the value function to guide the training process. A longer <code class="language-plaintext highlighter-rouge">e_len</code> (smaller <code class="language-plaintext highlighter-rouge">n_env</code>) often leads to better performance, albeit at the cost of lower token throughput.</p>

<h3 id="algorithm">Algorithm</h3>

<h4 id="dual-discounting-gae">Dual Discounting GAE</h4>

<div style="width: 100%;">
  <img src="images/algo.gif" style="width: 100%; height: auto;" />
</div>

<p>To incentivize agents to solve tasks with fewer environment steps, we decouple token-level discounting $$(\gamma_{\text{token}}, \lambda_{\text{token}})$$ and step-level $$(\gamma_{\text{step}}, \lambda_{\text{step}})$$. We set:</p>

<ul>
  <li>$$\gamma_{\text{step}} = 0.99$$, $$\lambda_{\text{step}} = 0.95$$</li>
  <li>$$\gamma_{\text{token}} = 1.0$$, $$\lambda_{\text{token}} = 1.0$$</li>
</ul>

<p>The GAE is computed recursively:</p>

<p>$$
\hat{A}<em>t = \gamma\lambda \hat{A}</em>{t+1} + \delta_t^V
$$</p>

<p>where:</p>

<ul>
  <li>$$\gamma\lambda = \gamma_{\text{step}} \lambda_{\text{step}}$$, if tokens are from different turns</li>
  <li>$$\gamma\lambda = \gamma_{\text{token}} \lambda_{\text{token}}$$, otherwise</li>
  <li>and $$\delta_t^V = -V(s_t) + r_t + \gamma V(s_{t+1})$$</li>
</ul>

<p>The recursion starts from the last token of the final turn and proceeds backward. Once all tokens in the final turn are processed, we move to the last token of the second-to-last turn, and continue this process recursively. During this process, all state tokens are skipped.
If a trajectory is truncated at step $$T$$, we store the next state $$s_{T+1}$$ but do not sample $$a_{T+1}$$. Instead, we use the final token of $$s_{T+1}$$ to estimate $$V(s_{T+1})$$, used as the bootstrap value in GAE.</p>

<h4 id="value-function-estimation">Value Function Estimation</h4>

<ul>
  <li>
    <p>When both $$\gamma$$ and $$\lambda$$ are set to 1.0, the value function serves purely as a baseline in PPO’s advantage estimation. Specifically, the advantage for the $$t$$-th token in the last turn is defined as $$A_{-1,t} = r - V_{-1,t}$$, where $$r$$ is the trajectory reward and $$V_{-1,t}$$ is the value estimate for the $$t$$-th token in the last turn.</p>
  </li>
  <li>
    <p>When $$\lambda$$ are less than 1.0, the value function contributes to the GAE objective beyond serving as a simple baseline. For instance, in our setting with $$\lambda_{\text{step}} = 0.95$$, $$\gamma_{\text{token}} = 1.0$$, $$\lambda_{\text{token}} = 1.0$$, and the reward $$r$$ that is zero along the trajectory except at the final turn, the advantage for the $$t$$-th token in the second-to-last turn is given by: $$A_{-2,t} = \gamma_{\text{step}}[\lambda_{\text{step}} r + (1-\lambda_{\text{step}}) V_{-1,0}] - V_{-2,t}$$. This indicates that, in our setting, the value function of the first token in each turn is used to bootstrap the GAE objective for the preceding turn.</p>
  </li>
  <li>
    <p>In our setting, the value of the first token of each turn carries more semantic significance than the subsequent tokens, we assign it a higher weight when training the critic network.</p>
  </li>
</ul>

<h4 id="critic-warmup">Critic Warmup</h4>

<p>In our setting, we warm up the critic before fine-tuning, as it is used both for bootstrapping truncated trajectories and for computing GAE. That is, we freeze the actor and update only the critic at the beginning of training. Specifically, We collect <code class="language-plaintext highlighter-rouge">w_epoch × batch_size</code> turns of data at the beginning. For each warmup iteration, we compute the GAE objective with current critic, sample one tenth of the collected data, train the critic, and repeat this process for <code class="language-plaintext highlighter-rouge">w_iter</code> iterations. We select <code class="language-plaintext highlighter-rouge">w_epoch = 40</code> and <code class="language-plaintext highlighter-rouge">w_iter = 5</code> in our experiments, and make sure that the critic loss converges to a small value before fine-tuning the actor.</p>

<h4 id="kl-divergence-in-reward">KL-Divergence in Reward</h4>

<p>Adding a KL-divergence term $$KL(\pi\mid\pi_0)$$ in reward stabilizes training. Without it, the policy quickly drifts from $$\pi_0$$ and converges to poor solutions. KL penalty encourage local exploration around $$\pi_0$$ before divergence. We observe an interesting observation related to the KL-Divergence:</p>

<ul>
  <li>Action Hacking: The LLM’s output can be decomposed into a reasoning path and a final action. We plot the average KL-divergence between $$\pi$$ and $$\pi_0$$ for both the reasoning path tokens and the final action tokens. A common failure mode in Crafter arises when the KL divergence of the final action tokens increases significantly faster than that of the reasoning path tokens. In this case, the agent learns to exploit easily accessible rewards early in training by modifying only the final action, without meaningfully improving its underlying reasoning. This leads to poor exploration.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Verlog solves most of the engineering challenges in building LLM agents for long-horizon, multi-turn tasks. Moving forward, we hope to use this framework to explore core research problems in LLM agents, such as memory design, exploration strategies, value function learning, and handling off-policyness.</p>

<blockquote>
  <p>If you find Verlog useful, please consider citing our workshop <a href="https://openreview.net/forum?id=6CE5PLsZdW">paper</a>. The full version is coming soon!</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
chen2025contextlite,
title={Context-lite Multi-turn Reinforcement Learning for {LLM} Agents},
author={Wentse Chen and Jiayu Chen and Hao Zhu and Jeff Schneider},
booktitle={ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models},
year={2025},
url={https://openreview.net/forum?id=6CE5PLsZdW}
}
</code></pre></div>  </div>
</blockquote>]]></content><author><name>Wentse Chen</name></author><category term="technique" /><summary type="html"><![CDATA[Source Code]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://agentic-intelligence-lab.org/images/logo.png" /><media:content medium="image" url="http://agentic-intelligence-lab.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Available Research Positions at the Agentic Intelligence Lab @ The University of Hong Kong</title><link href="http://agentic-intelligence-lab.org/2025/01/03/hiring-post.html" rel="alternate" type="text/html" title="Available Research Positions at the Agentic Intelligence Lab @ The University of Hong Kong" /><published>2025-01-03T00:00:00+00:00</published><updated>2025-08-15T02:45:07+00:00</updated><id>http://agentic-intelligence-lab.org/2025/01/03/hiring-post</id><content type="html" xml:base="http://agentic-intelligence-lab.org/2025/01/03/hiring-post.html"><![CDATA[<h2 id="about-the-pi">About the PI</h2>

<p>Jiayu Chen (陈佳玉) is an incoming Tenure-Track Assistant Professor at the <a href="https://www.dase.hku.hk/">Department of Data and Systems Engineering, The University of Hong Kong</a>. Currently, he is a Postdoctoral Fellow at the <a href="https://www.cs.cmu.edu/">School of Computer Science, Carnegie Mellon University</a>, working under the supervision of <a href="https://www.cs.cmu.edu/~schneide/">Professor Jeff Schneider</a> on data-driven profile control for nuclear fusion, as part of a joint research program with the <a href="https://control.princeton.edu/">Plasma Control Group, Princeton University</a>.</p>

<p>Dr. Chen earned his Ph.D. in Industrial Engineering and Operations Research from the <a href="https://engineering.purdue.edu/IE">Edwardson School of Industrial Engineering, Purdue University</a> and a Bachelor of Engineering from the <a href="https://www.coe.pku.edu.cn/">College of Engineering, Peking University</a>. During his Ph.D., he was co-advised by <a href="https://engineering.purdue.edu/CLANLabs">Professor Vaneet Aggarwal (Purdue University)</a>, an expert in theoretical reinforcement learning, and <a href="https://www2.seas.gwu.edu/~tlan/">Professor Tian Lan (George Washington University)</a>, an expert in multi-agent systems.</p>

<p>He is the director of the <a href="https://agentic-intelligence-lab.org/">Agentic Intelligence Lab</a>, where his research focuses on Learning for Sequential Decision-Making and Continuous Control. Dr. Chen has published extensively in top venues such as NeurIPS, ICML, ICRA, ICAPS, AAMAS, TMLR, and IEEE TNNLS as the first author. His work has been recognized with prestigious awards, including the Oracle Research Award and the Purdue Research Grant.</p>

<h2 id="about-the-universitydepartment">About the University/Department</h2>

<p>Founded in 1911, the University of Hong Kong (HKU) is the oldest tertiary education institution in Hong Kong, renowned globally for its excellence in teaching, research, and innovation. According to the QS World University Rankings, HKU is currently ranked <strong>2nd in Asia</strong> and <strong>17th worldwide.</strong> Studying at HKU offers unique advantages: its location in the vibrant, internationally connected city of Hong Kong provides unmatched access to global business networks, a multicultural environment, and close proximity to the thriving economies of the Greater Bay Area.</p>

<p>In the midst of the artificial intelligence and big data revolution, the <a href="https://www.dase.hku.hk/about-us/introduction">Department of Data and Systems Engineering (DASE)</a> upholds the philosophy of “computing-in-practice”. The vision is to bridge the cyber and physical worlds by integrating data science methodologies, such as artificial intelligence, with human tools like robots to create greater value for humanity. To achieve this goal, DASE focuses on three key areas: <strong>Data Engineering and Analytics</strong>, <strong>Human-Machine Cooperation</strong>, and <strong>Robotics and Automation</strong>.</p>

<h2 id="research-areas">Research Areas</h2>

<p>The Agentic Intelligence Lab conducts broad research across Machine Learning, Robotics, and Operations Research. At present, our work focuses on advancing the following key areas:</p>

<ul>
  <li>Theoretical foundations, algorithm design, and applications of (Multi-agent) Reinforcement Learning;</li>
  <li>Developing a unified theoretical framework for Reinforcement Learning, Optimal Control, and Stochastic Optimization;</li>
  <li>Scaling Laws in sequential decision-making, with a special emphasis on efficient (Multi-agent) Monte Carlo Tree Search algorithms and advanced Reasoning Models;</li>
  <li>Robotic Foundation Models;</li>
  <li>End-to-end multi-modal learning for Humanoid Robotic Control and its industrial applications;</li>
  <li>Post-training techniques for Large Language Model agents and their applications in sequential decision-making;</li>
  <li>Artificial General Intelligence grounded in the principles of <a href="http://www.incompleteideas.net/Talks/AlbertaPlan.pdf">The Alberta Plan</a>.</li>
</ul>

<h2 id="available-positions">Available Positions</h2>

<p>We have the following openings:</p>

<!-- - **Two fully funded Ph.D. positions** starting in **Fall 2025 (as early as July)**
- Openings for **postdoctoral researchers**, **self-financed Ph.D. students**, **part-time Ph.D. students**, **research assistants**, and **visiting scholars** are available **year-round**  -->

<ul>
  <li><strong>Fully funded Ph.D. positions</strong> starting in <strong>Fall 2025</strong> have been taken.</li>
  <li>Openings for <strong>self-financed Ph.D. students</strong> and  <strong>research assistants</strong> are available <strong>year-round</strong>.</li>
  <li>❗❗❗ We have an available postdoc position in reinforcement learning and robotic learning.</li>
  <li>❗❗❗ We need a <strong>remote research intern</strong> for a project to benchmark offline RL and control methods for plasma control in nuclear fusion. Please check this <a href="https://github.com/LucasCJYSDL/Offline-RL-Kit-for-Nuclear-Fusion">repo</a> for details and reach out to Dr. Jiayu Chen if interested.</li>
</ul>

<p>Ph.D. and postdoctoral applicants are expected to have <strong>strong proficiency in mathematics or programming, along with demonstrated self-motivation and resilience</strong>. Candidates with expertise in the following areas are highly preferred: Humanoid Robots, Large Language/Reasoning Models, Control Theory, Optimization Theory, Statistical Machine Learning.</p>

<p>We offer competitive remuneration. For Ph.D. students, the basic level of scholarship is 18,760 HK dollars per month. For <a href="https://www.hr.hku.hk/career_opportunities/pdf-rap.html">postdocal researchers</a>, the basic salary is no less than 30,000 HK dollars per month. Outstanding candidates are encouraged to apply for the <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme">HKU Presidential PhD Scholar Programme (HKUPS)</a>, <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hong_kong_phd_fellowship_scheme">Hong Kong PhD Fellowship Scheme (HKPFS)</a>, and <a href="https://www.ugc.edu.hk/eng/rgc/funding_opport/pdfs/">RGC Postdoctoral Fellowship Scheme (PDFS)</a> to get financial support of approximately 430,000 HK dollars annaly.</p>

<p>We also provide opportunities for visiting experiences at top U.S. universities such as UC Berkeley, UIUC, CMU, Purdue, and GWU. For <strong>research assistants</strong>, we offer strong recommendation letters and ample opportunities to publish papers in top machine learning and robotics venues.</p>

<p>For detailed requirements and additional information, please refer to the respective categories in <a href="https://agentic-intelligence-lab.org/openings/">openings</a>.  If interested, please contact <a href="https://agentic-intelligence-lab.org/members/jiayu-chen.html">Dr. Jiayu Chen</a> directly through <u>jiayuc@hku.hk</u>. The Ph.D. application system at HKU accepts submissions <strong>year-round</strong>.</p>

<p><u>Deeply sorry that I am unable to respond to every inquiry unless it is to send an interview invitation.</u></p>]]></content><author><name>Jiayu Chen</name></author><category term="hiring" /><summary type="html"><![CDATA[About the PI]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://agentic-intelligence-lab.org/images/logo.png" /><media:content medium="image" url="http://agentic-intelligence-lab.org/images/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>